"""
File containing the definition of Graph Neural Network (GNN) models.

This module defines two GNN models: SkipGCN and GCN. Both models are designed for graph classification tasks.
"""

import numpy as np
import torch  # type: ignore
import torch.nn.functional as F
from omegaconf import DictConfig
from torch.nn import BatchNorm1d, Linear, Module
from torch_geometric.nn import global_max_pool
from torch_geometric.nn.conv import GCNConv
from torch_geometric.nn.pool.glob import global_max_pool as gmp
from torch_geometric.nn.pool.glob import global_mean_pool as gap


class SkipGCN(Module):
    """
    SkipGCN model class derived from GCN with skip connections.

    This class implements a Graph Convolutional Network (GCN) with skip connections,
    which allows the model to capture both local and global graph structures.

    Parameters
    ----------
    model_params : DictConfig
        Dictionary containing the model configuration, including parameters like hidden_channels and dropout.
    num_node_features : int
        Number of features in the nodes of the graph.

    Attributes
    ----------
    embeddings : np.ndarray
        Array to store the embeddings generated by the model.
    drop_p : float
        Dropout probability used in the model.
    conv1, conv2, conv3 : GCNConv
        Convolutional layers for the GCN.
    bn_conv1, bn_conv2, bn_conv3 : BatchNorm1d
        Batch normalization layers for the convolutional outputs.
    bn1 : BatchNorm1d
        Batch normalization layer for the final embeddings.
    lin : Linear
        Linear layer for the final classification.

    Methods
    -------
    forward(x, edge_index, edge_weight, pooling_type, batch)
        Forward pass of the SkipGCN model.
    get_embeddings()
        Returns the embeddings generated by the model.
    """

    def __init__(self, model_params: DictConfig, num_node_features: int):
        hidden_channels = model_params['hidden_channels']
        # Default values for embeddings
        self.embeddings = np.ones(2 * hidden_channels + num_node_features)
        self.drop_p = model_params['dropout']
        super(SkipGCN, self).__init__()
        self.conv1 = GCNConv(num_node_features,
                             hidden_channels,
                             normalize=False)
        # Skip connection: hidden_channels + num_node_features for residuals
        self.conv2 = GCNConv(hidden_channels + num_node_features,
                             hidden_channels + num_node_features,
                             normalize=False)
        # Skip connection: hidden_channels + hidden_channels + residuals
        self.conv3 = GCNConv(2 * hidden_channels + num_node_features,
                             2 * hidden_channels + num_node_features,
                             normalize=False)
        self.bn_conv1 = BatchNorm1d(num_features=hidden_channels)
        self.bn_conv2 = BatchNorm1d(num_features=hidden_channels + num_node_features)
        self.bn_conv3 = BatchNorm1d(num_features=2 * hidden_channels + num_node_features)
        self.bn1 = BatchNorm1d(num_features=2 * hidden_channels + num_node_features)
        self.lin = Linear(in_features=2 * hidden_channels + num_node_features, out_features=1)

    def forward(self,
                x: torch.Tensor,
                edge_index: torch.Tensor,
                edge_weight: torch.Tensor,
                pooling_type: str,
                batch: torch.Tensor) -> torch.Tensor:
        residual_0 = x
        # 1. Obtain node embeddings
        x = self.conv1(x, edge_index, edge_weight)
        x = x.relu()
        x = self.bn_conv1(x)
        x = F.dropout(x, p=self.drop_p)
        residual_1 = x
        x = torch.cat([x, residual_0], dim=1)
        x = self.conv2(x, edge_index, edge_weight)
        x = x.relu()
        x = self.bn_conv2(x)
        x = F.dropout(x, p=self.drop_p)
        x = torch.cat([x, residual_1], dim=1)
        x = self.conv3(x, edge_index, edge_weight)
        x = self.bn_conv3(x)
        # 2. Readout layer
        # Global Pooling (stack different aggregations)
        #x = eval(f'{pooling_type}')(x, batch=batch)
        x = global_max_pool(x, batch=batch)
        # Batch norm for embeddings
        x = self.bn1(x)
        self.embeddings = x.detach().to('cpu').numpy()

        # 3. Apply a final classifier
        x = self.lin(x)
        return x

    def get_embeddings(self) -> np.ndarray:
        return self.embeddings


class GCN(Module):
    """
    GCN model class derived from GCN.

    This class implements a basic Graph Convolutional Network (GCN) for graph classification tasks.

    Parameters
    ----------
    config : DictConfig
        Dictionary containing the model configuration, including parameters like embedding_size.
    num_node_features : int
        Number of features in the nodes of the graph.

    Attributes
    ----------
    embeddings : np.ndarray
        Array to store the embeddings generated by the model.
    conv1, conv2, conv3 : GCNConv
        Convolutional layers for the GCN.
    bn1 : BatchNorm1d
        Batch normalization layer for the final embeddings.
    lin : Linear
        Linear layer for the final classification.

    Methods
    -------
    forward(x, edge_index, edge_weight, batch)
        Forward pass of the GCN model.
    get_embeddings()
        Returns the embeddings generated by the model.
    """

    def __init__(self, config: DictConfig, num_node_features: int):
        hidden_channels = config.training['embedding_size']
        # Default values for embeddings
        self.embeddings = np.ones(2 * hidden_channels)
        super(GCN, self).__init__()
        self.conv1 = GCNConv(num_node_features, hidden_channels, normalize=False)
        self.conv2 = GCNConv(hidden_channels, hidden_channels, normalize=False)
        self.conv3 = GCNConv(hidden_channels, hidden_channels, normalize=False)
        self.bn1 = BatchNorm1d(num_features=hidden_channels)
        self.lin = Linear(in_features=hidden_channels, out_features=1)

    def forward(self,
                x: torch.Tensor,
                edge_index: torch.Tensor,
                edge_weight: torch.Tensor,
                batch: torch.Tensor) -> torch.Tensor:
        # 1. Obtain node embeddings
        x = self.conv1(x, edge_index, edge_weight)
        x = x.relu()
        x = self.conv2(x, edge_index, edge_weight)
        x = x.relu()
        x = self.conv3(x, edge_index, edge_weight)

        # 2. Readout layer
        # Global Pooling (stack different aggregations)
        x = gmp(x, batch)
        # Batch norm for embeddings
        x = self.bn1(x)
        self.embeddings = x.detach().to('cpu').numpy()

        # 3. Apply a final classifier
        x = self.lin(x)
        return x

    def get_embeddings(self) -> np.ndarray:
        return self.embeddings
